import os
import time
import json
import tqdm
from elasticsearch import Elasticsearch
from elasticsearch.helpers import streaming_bulk, BulkIndexError
from dotenv import load_dotenv

"""
load_dotenv("./elastic/.env")
elastic_pwd = os.getenv("ELASTIC_PASSWORD")
certs_path = os.getenv("CERTS_PATH")

es_client = Elasticsearch("https://localhost:9200",
    basic_auth=("elastic", elastic_pwd),ca_certs=certs_path)
"""

# TOOLS =======================================================
def count_files(path):
    total_files = 0
    for root, dirs, files in os.walk(path):
        total_files += len(files)
    return total_files

def convert_x_generator_format(document):
    """
    Fix the json format for x_generator to avoid bulk crashing
    """
    if 'x_generator' in document:
        # Check if 'x_generator' is a dictionary (Case A)
        if isinstance(document['x_generator'], dict):
            return document  # Already in Case A format
        else:  # 'x_generator' is a string (Case B)
            # Convert Case B to Case A format
            new_x_generator = {
                "engine": document['x_generator']
            }
            document['x_generator'] = new_x_generator
            return document
    else:
        return document  # 'x_generator' field does not exist

# GET FUNCTIONS ===============================================
def get_cve(cve_path:str, year:str, id:str) -> dict:
    # Will return the json content of the cve that has the associated ID
    # If the cve is not found, it will return an empty string

    data = ""

    cve_path = os.path.join(cve_path, year)

    folder_name = f"{id[:-3]}xxx" # cvelistV5 norm
    filename = f"CVE-{year}-{id}.json"
    file_path = os.path.join(cve_path, folder_name, filename)
    with open(file_path, 'r') as file:
        data = json.load(file)
    return data

def get_cve_path(cve_path:str, year:str, id:str) -> dict:
    # Will return the cve path for a specific cve

    cve_path = os.path.join(cve_path, year)
    folder_name = f"{id[:-3]}xxx" # cvelistV5 norm
    filename = f"CVE-{year}-{id}.json"
    file_path = os.path.join(cve_path, folder_name, filename)
    
    return file_path

def get_all_cve_by_year(cve_path:str, year:str) -> list:
    # Will return the json object of all cve in the given path

    documents = []
    cve_path = os.path.join(cve_path, year)
    for folder in os.listdir(cve_path):
        folder_path = os.path.join(cve_path, folder)
        if os.path.isdir(folder_path):
            for cvefile in os.listdir(folder_path):
                file_path = os.path.join(folder_path, cvefile)
                with open(file_path, 'r') as file:
                    data = json.load(file)
                    documents.append(data)
    return documents

def get_all_cve(cve_path:str,number_of_files:int) -> list:
    # Will return the json object of all cve in the given path
    
    documents = []
    print("Reading content of all documents :")
    progress = tqdm.tqdm(unit="files", total=number_of_files)
    for year in os.listdir(cve_path):
        cve_path_year = os.path.join(cve_path,year)
        if os.path.isdir(cve_path_year):
            for folder in os.listdir(cve_path_year):
                folder_path = os.path.join(cve_path_year, folder)
                if os.path.isdir(folder_path):
                    for cvefile in os.listdir(folder_path):
                        file_path = os.path.join(folder_path, cvefile)
                        with open(file_path, 'r') as file:
                            data = json.load(file)
                            id = cvefile.split(".")[0]
                            documents.append((id,data))
                            progress.update(1)
    print("Reading ended")
    return documents

# BULK FUNCTIONS ==============================================
def bulk_data(json_files:list,ELASTICSEARCH_INDEX:str):
    for index, document in json_files:
        # convert_x_generator_format(document)
        yield {
            "_index": ELASTICSEARCH_INDEX,
            "_id" : index,
            "_source": document,
        }

def bulk_json_files(es_client:Elasticsearch,json_files:list,ELASTICSEARCH_INDEX:str,num_files:int):
    """
    Bulk insert documents into Elasticsearch
    """
    progress = tqdm.tqdm(unit="docs", total=num_files)
    errors = []
    id_failed = []
    success, failed = 0, 0
    stream = bulk_data(json_files,ELASTICSEARCH_INDEX)
    print("Bulking documents to Elasticsearch :")
    try:
        for ok, item in streaming_bulk(es_client, actions=stream, raise_on_error=False):
            progress.update(1)
            if not ok:
                if 'index' in item and 'error' in item['index']:
                    error_reason = item['index']['error']['reason']
                    # error_field = error_reason.split("object mapping for ")[1].split("]")[0]
                    # ignored_field = error_field.split(".")[-1]
                    print(f"Ignored field '{error_reason}' due to parsing error.")
                    id_failed.append(item['index']['_id'])
                errors.append(item)
                failed += 1
            else:
                success += 1
    except BulkIndexError as e:
        errors.extend(e.errors)
        failed += len(e.errors)
    return success, failed, errors, id_failed

# ELK QUERIES =================================================
def create_index(ELASTICSEARCH_INDEX:str,es_client:Elasticsearch):
    """
    Create Elasticsearch index without specifying mappings
    """
    print(es_client.indices.create(index=ELASTICSEARCH_INDEX))
  
def delete_all_documents(ELASTICSEARCH_INDEX):
    query = {
    "query": {
        "match_all": {}  # Match all documents
    }}
    response = es_client.delete_by_query(index=ELASTICSEARCH_INDEX, body=query)

def get_all_documents(ELASTICSEARCH_INDEX):
    """
    Retrieve all documents from the specified index
    """
    # Define the search query to retrieve all documents
    search_query = {
        "query": {
            "match_all": {}  # Match all documents
        }
    }

    # Execute the search query
    search_results = es_client.search(index=ELASTICSEARCH_INDEX, body=search_query)

    # Extract and return the documents
    documents = [hit['_source'] for hit in search_results['hits']['hits']]
    formatted_data = json.dumps(documents, indent=4)
    print(formatted_data)
    return documents