import os
# import time
# import requests
# import zipfile
import pandas as pd
from datetime import datetime
from elasticsearch import Elasticsearch
from elasticsearch import helpers
es_client = Elasticsearch(http_compress=True)

"""
from sqlalchemy import create_engine
from dotenv import load_dotenv
from sqlalchemy.engine.base import Engine

load_dotenv()
port = os.getenv("POSTGRES_PORT")
base_pg = os.getenv("POSTGRES_DB")
host = os.getenv("POSTGRES_HOST")
user = os.getenv("POSTGRES_USER")
password = os.getenv("POSTGRES_PASSWORD")
"""

# timestr = time.strftime("%Hh%M-%d/%m/%Y")
# pd.set_option('display.max_columns', 500)

def get_df(cve_path:str, year:str) -> pd.DataFrame:
    df = None
    cve_path = os.path.join(cve_path, year)
    """create dataframe from file"""
    for folder in os.listdir(cve_path):
        folder_path = os.path.join(cve_path, folder)
        if os.path.isdir(folder_path):
            for cvefile in os.listdir(folder_path):
                df = pd.read_json(os.path.join(folder_path, cvefile))

    print(df)
    #df = pd.read_json(f"{cve_path}/")
    # df["bulk_date"] = timestr
    return df

def get_cve(cve_path:str, year:str, id:str) -> pd.DataFrame:
    # This function will return the panda dataframe of the cve that has the associated ID
    # If the cve is not found, it will return None

    df = None
    cve_path = os.path.join(cve_path, year)

    folder_name = f"{id[:-3]}xxx" # cvelistV5 norm
    filename = f"CVE-{year}-{id}.json"
    df = pd.read_json(os.path.join(cve_path, folder_name, filename))

    return df

def check_cve(df:pd.DataFrame) -> bool:
    # This function will check if the dataframe is valid and the cve contains the good informations
    return \
        {'dataType', 'dataVersion', 'cveMetadata', 'containers'}.issubset(df.columns) and \
        df['containers']['cna'] is not None

def clean_cve(df:pd.DataFrame) -> pd.DataFrame:
    # Set date by default if not in database
    default_date = datetime(2024, 1, 1, 0, 0) # we could change to 1970
    if not df['cveMetadata']['dateUpdated']:
        df['cveMetadata']['dateUpdated'].apply(default_date)
    if not df['cveMetadata']['dateReserved']:
        df['cveMetadata']['dateReserved'].apply(default_date)
    if not df['cveMetadata']['datePublished']:
        df['cveMetadata']['datePublished'].apply(default_date)

    # Remove NaN values
    # df['cveMetadata']['cna'] = 0

    # TODO Have to check if we can remove the NaN of the 'containers' collumn

    return df

def bulk_dataframe(df:pd.DataFrame):
    df_iter = df.iterrows()
    for index, document in df_iter:
        yield {
            "_index": 'index', # TODO chose what index to give
            "_type": "_doc", # because type is being deprecated by Elasticsearch
            "_id" : f"{document['cveId']}",
            "_source": document.to_dict(),
        }
    raise StopIteration

def main():
    current_path = os.path.dirname(os.getcwd())

    #cve_path = f"{current_path}/cvelistV5/cves"
    cve_path = f"{current_path}\\cvelistV5\\cves"
    print(cve_path)
    #get_df(cve_path, 2024)
    df = get_cve(cve_path, "2019", "1020001")
    #print(df)
    print(check_cve(df))
    df = clean_cve(df)
    #print(df)
    
    helpers.bulk(es_client, bulk_dataframe(df))

    # df = pd.read_json(os.path.join(f"{cut}"))
    # if not os.path.exists(current_path):
    #     os.makedirs(current_path)
    # cve_path = ""
    #  = (current_path)
    # if .get_data():
    #     print("Dowload successed")
    # else:
    #     print("Dowload failed")
    # df = .get_df()
    #engine = create_engine(f"postgresql://{user}:{password}@{host}:{port}/{base_pg}")
    # print("upload succesfull :",df.to_sql("",engine,if_exists="fail",index=False,schema="odata") > 0)

if __name__ == "__main__":
    main()